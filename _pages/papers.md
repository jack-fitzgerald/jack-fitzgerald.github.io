---
title: 
layout: single
classes: wide
permalink: /papers/
---
<br/> 

# <center> Job Market Paper </center>
- - -

**The Need for Equivalence Testing in Economics**. [<i>Institute for Replication Discussion Paper Series</i>, No. 125](https://hdl.handle.net/10419/296190), 2024. Under submission. <br/>
<small>[ <a href="#/" onclick="visib('equiv-test')">Abstract</a> | [Draft](https://jack-fitzgerald.github.io/files/The_Need_for_Equivalence_Testing_in_Economics.pdf) | [Online Appendix](https://jack-fitzgerald.github.io/files/The_Need_for_Equivalence_Testing_in_Economics_Online_Appendix.pdf) | [Twitter/X Thread](https://x.com/FitzgeraldJack_/status/1799091059802149266) | [30-Minute Presentation](https://youtu.be/ltkuhpcH9mA) | [Interview: Economisch Statistische Berichten (in Dutch)](https://esb.nu/we-moeten-vaker-toegeven-dat-er-niks-te-concluderen-valt/) ] </small>

<div id="equiv-test" style="display: none; text-align: justify; line-height: 1.2" ><small>

Equivalence testing can provide statistically significant evidence that economic relationships are practically negligible. I demonstrate its necessity in a large-scale replication of estimates defending 135 null claims made in 81 articles from top economics journals. 36-63% of estimates defending the average null claim fail lenient equivalence tests. Obtaining equivalence testing failure rates that surveyed researchers deem acceptable requires arguing that nearly 75% of published estimates in economics are practically equal to zero, implying that Type II error rates are unacceptably high throughout economics. I provide economists with guidelines and commands in Stata/R for conducting credible equivalence testing and practical significance testing in future research.

</small><br><br/></div>

# <center> Published and Forthcoming Articles </center>
- - -

**US States That Mandated COVID-19 Vaccination See Higher, Not Lower, Take-Up of COVID-19 Boosters and Flu Vaccines**. *Proceedings of the National Academy of Sciences* (121)41, e2403758121, 2024. <br/>
<small>[ <a href="#/" onclick="visib('pnas_replication')">Abstract</a> | [Article (Open Access)](https://doi.org/10.1073/pnas.2403758121) | [Draft](https://jack-fitzgerald.github.io/files/RR24_Replication.pdf) | [Data & Code, Published Replication](https://osf.io/mdfb4/) | [Reply](https://www.pnas.org/doi/10.1073/pnas.2409246121) | [Response to Reply](https://jack-fitzgerald.github.io/files/RR24_Response_to_Reply.pdf) | [Data & Code, Response to Reply](https://osf.io/9cn38/) | [Twitter/X Thread](https://twitter.com/FitzgeraldJack_/status/1841491274391597266) ] </small>

<div id="pnas_replication" style="display: none; text-align: justify; line-height: 1.2" ><small>

Rains & Richards (2024, <i>Proceedings of the National Academy of Sciences</i>) find that compared to US states that instituted bans on COVID-19 vaccination requirements, states that imposed COVID-19 vaccination mandates exhibit lower adult and child uptake of flu vaccines, and lower uptake of COVID-19 boosters. These differences are generally interpreted causally. However, further inspection reveals that these results are driven by the inclusion of a single bad control variable. When removed, the data instead shows that states which mandated COVID-19 vaccination experience higher COVID-19 booster and flu vaccine takeup than states that banned COVID-19 vaccination requirements.

</small><br><br/></div>

**Is There a Foreign Language Effect on Workplace Bribery Susceptibility? Evidence from a Randomized Controlled Vignette Experiment** (with [Paul Stroet](https://paulstroet.netlify.app/), [Arjen van Witteloostuijn](https://research.vu.nl/en/persons/arjen-van-witteloostuijn), and [Kristina S. Weißmüller](https://www.ksweissmueller.com/)). *Journal of Business Ethics*, 2024. <br/>
<small>[ <a href="#/" onclick="visib('fle_bribery')">Abstract</a> | [Article (Open Access)](https://doi.org/10.1007/s10551-024-05731-x) | [Draft](https://jack-fitzgerald.github.io/files/JBE_manuscript.pdf) | [Code](https://doi.org/10.17605/OSF.IO/Y3NQ7) ] </small>

<div id="fle_bribery" style="display: none; text-align: justify; line-height: 1.2" ><small>

Theory and evidence from the behavioral science literature suggest that the widespread and rising use of <i>lingua francas</i> in the workplace may impact the ethical decision-making of individuals who must use foreign languages at work. We test the impact of foreign language usage on individual susceptibility to bribery in workplace settings using a vignette-based randomized controlled trial in a Dutch student sample. Results suggest that there is not even a small foreign language effect on workplace bribery susceptibility. We combine traditional null hypothesis significance testing with equivalence testing methods novel to the business ethics literature that can provide statistically significant evidence of bounded or null relationships between variables. These tests suggest that the foreign language effect on workplace bribery susceptibility is bounded below even small effect sizes. <i>Post hoc</i> analyses provide evidence suggesting fruitful further routes of experimental research into bribery.

</small><br><br/></div>

# <center> Working Papers </center>
- - -

**Imputations, Inverse Hyperbolic Sines, and Impossible Values**. 2024. Under invited submission, *Nature Human Behaviour*. <br/>
<small>[ <a href="#/" onclick="visib('WEA23')">Abstract</a> | [Data & Code](https://osf.io/hce6n/) | [Draft](https://jack-fitzgerald.github.io/files/WEA23_Replication.pdf)] </small>

<div id="WEA23" style="display: none; text-align: justify; line-height: 1.2" ><small>

Wolfowicz et al. (2023, <i>Nature Human Behaviour</i>) find that more arrests and convictions for terrorism offenses decrease terrorism, more charges increase terrorism, and longer sentences do not deter terrorism in 28 European Union member states from 2006-2021. I assess the computational reproducibility of their study and find many data irregularities. The article's primary dependent variable - purportedly an inverse hyperbolic sine transformation of terrorist attack rates - takes on 292 different values when attack rates equal zero, and negatively correlates with attack rates. Many variables exhibit impossible values or undisclosed imputations, often masking a lack of reporting in the article's main data sources. I estimate that the authors have access to 57% fewer observations than claimed. Reproduction attempts produce estimates at least 77.7% smaller than the published estimates. Models reflecting the true degree of missing data produce estimates that are not statistically significantly different from zero for any independent variable of interest.

</small><br><br/></div>

**Revisiting the Impacts of Anti-Discrimination Employment Protections on American Businesses**. 2024. Under submission, <i>Management Science</i>.
<small>[ <a href="#/" onclick="visib('GS22')">Abstract</a> | [Code & Data Retrieval Instructions](https://osf.io/6q4k5/) | [Draft](https://jack-fitzgerald.github.io/files/GS22_Replication.pdf)] </small>

<div id="GS22" style="display: none; text-align: justify; line-height: 1.2" ><small>

Greene & Shenoy (2022, <i>Management Science</i>) - henceforth GS22 - find that the staggered adoption of U.S. state-level protections against racial discrimination in employment decreased both the profitability and leverage of affected businesses. However, these results arise from two-way fixed effects (TWFE) difference-in-differences models. Such models are now known to return inaccurate estimates of average treatment effects on the treated (ATTs) when treatment assignment is staggered, as some firm-year ATTs can enter the TWFE estimator with negative weight. I find that 21-36% of firm-year ATTs in GS22's sample enter the TWFE estimator with negative weight. I then replicate GS22's results using recently-developed difference-in-differences estimators that return valid ATT estimates under staggered adoption. None of these new ATT estimates are statistically significantly different from zero.

</small><br><br/></div>

**The Problems with Poor Proxies: Does Innovation Mitigate Agricultural Damage from Climate Change?** [<i>Institute for Replication Discussion Paper Series</i>, No. 158](https://www.econstor.eu/handle/10419/303190), 2024. <br/>
<small>[ <a href="#/" onclick="visib('MS23')">Abstract</a> | [Draft](https://jack-fitzgerald.github.io/files/MS23_Replication.pdf) | [Data & Code](https://osf.io/d7wz9/) | [Authors' Response](https://www.econstor.eu/handle/10419/303191) | [Twitter/X Thread](https://threadreaderapp.com/thread/1844359711854104708.html) ] </small>

<div id="MS23" style="display: none; text-align: justify; line-height: 1.2" ><small>

Moscona & Sastry (2023, <i>Quarterly Journal of Economics</i>) - henceforth MS23 - find that cropland values are significantly less damaged by extreme heat exposure (EHE) when crops are more exposed to technological innovation. However, MS23's 'innovation exposure' variable does not measure innovation, instead proxying innovation using a measure of crops' national heat exposure. A re-examination of MS23's replication data - which permits a close but inexact reproduction of MS23's published findings - shows that this proxy moderates EHE impacts for reasons unrelated to innovation. The proxy is practically identical to local EHE, so MS23's models examining interaction effects between their proxy and local EHE effectively interact local EHE with itself. I document extensive evidence that MS23's findings on 'innovation exposure' are simply artefacts of nonlinear impacts in local EHE, and uncover robustness issues for other key findings. I then construct direct measures of innovation exposure from MS23's crop variety and patenting data. Replacing MS23's proxy with these direct innovation measures decreases MS23's moderating effect estimates by at least 99.8% in standardized units; none of these new estimates are statistically significantly different from zero. Similar results arise from an instrumental variables strategy that instruments my direct innovation measures with MS23's heat proxy. These results cast doubt on the general capacity for market innovations to mitigate agricultural damage from climate change.

</small><br><br/></div>

**Manipulation Tests in Regression Discontinuity Design: The Need for Equivalence Testing**. [<i>Institute for Replication Discussion Paper Series</i>, No. 136](https://hdl.handle.net/10419/300277), 2024. Under submission. <br/>
<small>[ <a href="#/" onclick="visib('rdd-equiv')">Abstract</a> | [Draft](https://jack-fitzgerald.github.io/files/RDD_Equivalence.pdf) | [Twitter/X Thread](https://x.com/FitzgeraldJack_/status/1815334145091920105)] </small>

<div id="rdd-equiv" style="display: none; text-align: justify; line-height: 1.2" ><small>

Researchers utilizing regression discontinuity design (RDD) commonly test for running variable (RV) manipulation around a cutoff, but incorrectly assert that insignificant manipulation test statistics are evidence of negligible manipulation. I introduce simple frequentist equivalence testing procedures that can provide statistically significant evidence that RV manipulation around a cutoff is practically equivalent to zero. I then demonstrate the necessity of these procedures, leveraging replication data from 36 RDD publications to conduct 45 equivalence-based RV manipulation tests. Over 44% of RV density discontinuities at the cutoff can not be significantly bounded beneath a 50% upward jump. Bounding equivalence-based manipulation test failure rates beneath 5% requires arguing that a 350% upward density jump is practically equivalent to zero. Meta-analytic estimates reveal that average RV manipulation around the cutoff is equivalent to a 26% upward density jump. These results imply that many published RDD estimates may be confounded by discontinuities in potential outcomes due to RV manipulation that remains undetectable by existing tests. I provide research guidelines and commands in Stata and R to help researchers conduct more credible equivalence-based manipulation testing in future RDD research.

</small><br><br/></div>

**Identifying the Impact of Hypothetical Incentives on Experimental Outcomes and Treatment Effects**. 2024. <br/>
<small>[ <a href="#/" onclick="visib('hypo-bias')">Abstract</a> | [Code & Data Retrieval Instructions](https://osf.io/fe6jn/) | [Draft](https://jack-fitzgerald.github.io/files/Hypothetical_Bias.pdf)] </small>

<div id="hypo-bias" style="display: none; text-align: justify; line-height: 1.2" ><small>

Recent studies showing that some outcome variables do not statistically significantly differ between real-stakes and hypothetical-stakes conditions have raised methodological challenges to experimental economics' disciplinary norm that experimental choices should be incentivized with real stakes. I show that the hypothetical bias measures estimated in these studies do not econometrically identify the hypothetical biases that matter in most modern experiments. Specifically, traditional hypothetical bias measures are fully informative in 'elicitation experiments' where the researcher is uninterested in treatment effects (TEs). However, in 'intervention experiments' where TEs are of interest, traditional hypothetical bias measures are uninformative; real stakes matter if and only if TEs differ between stakes conditions. I demonstrate that traditional hypothetical bias measures are often misleading estimates of hypothetical bias for intervention experiments, both econometrically and through re-analyses of three recent hypothetical bias experiments. The fact that a given experimental outcome does not statistically significantly differ on average between stakes conditions does not imply that all TEs on that outcome are unaffected by hypothetical stakes. Therefore, the recent hypothetical bias literature does not justify abandoning real stakes in most modern experiments. Maintaining norms that favor completely or probabilistically providing real stakes for experimental choices is useful for ensuring externally valid TEs in experimental economics.

</small><br><br/></div>

# <center> Selected Works in Progress </center>
- - -

**The Impacts of Replications in Economics** (with [Bob Reed](https://profiles.canterbury.ac.nz/Bob-Reed) and [Tom Coupé](https://profiles.canterbury.ac.nz/Tom-Coupe)).

**Practical Significance Testing in Psychological Research: A Tutorial in Three-Sided Testing** (with [Peder Isager](https://pedermisager.org/)).

**Fine Enough or Don't Fine At All? Experimental Evidence on Optimal Fine Levels for Natural Resource Conservation**.

**The Nuclear Option: Cross-Country Evidence on the Impacts of Nuclear Power Supply**.

[//]: This java script is the button to show abstract
<script>
 function visib(id) {
  var x = document.getElementById(id);
  if (x.style.display === "block") {
    x.style.display = "none";
  } else {
    x.style.display = "block";
  }
}
</script>

[//]:&emsp;<button onclick="visib('polariz')" class="btn btn--inverse btn--small">Abstract</button>
